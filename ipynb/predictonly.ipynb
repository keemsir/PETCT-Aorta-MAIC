{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_final_checkpoint.model',\n",
       " 'progress.png',\n",
       " 'validation_raw',\n",
       " 'model_best.model.pkl',\n",
       " '.ipynb_checkpoints',\n",
       " 'validation_raw_postprocessed',\n",
       " 'model_final_checkpoint.model.pkl',\n",
       " 'model_best.model',\n",
       " 'postprocessing.json',\n",
       " 'training_log_2021_10_26_06_09_32.txt',\n",
       " 'debug.json']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Union, Tuple\n",
    "import numpy as np\n",
    "os.listdir('/mnt/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy model..\n",
    "# shutil.copytree('/mnt/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/', './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = './input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "files_ = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def subfiles(folder: str, join: bool = True, prefix: str = None, suffix: str = None, sort: bool = True) -> List[str]:\n",
    "    if join:\n",
    "        l = os.path.join\n",
    "    else:\n",
    "        l = lambda x, y: y\n",
    "    res = [l(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))\n",
    "           and (prefix is None or i.startswith(prefix))\n",
    "           and (suffix is None or i.endswith(suffix))]\n",
    "    if sort:\n",
    "        res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", '--input_folder', help=\"Must contain all modalities for each patient in the correct\"\n",
    "                                                     \" order (same as training). Files must be named \"\n",
    "                                                     \"CASENAME_XXXX.nii.gz where XXXX is the modality \"\n",
    "                                                     \"identifier (0000, 0001, etc)\", required=True)\n",
    "    parser.add_argument('-o', \"--output_folder\", required=True, help=\"folder for saving predictions\")\n",
    "    parser.add_argument('-t', '--task_name', help='task name or task ID, required.',\n",
    "                        default=default_plans_identifier, required=True)\n",
    "\n",
    "    parser.add_argument('-tr', '--trainer_class_name',\n",
    "                        help='Name of the nnUNetTrainer used for 2D U-Net, full resolution 3D U-Net and low resolution '\n",
    "                             'U-Net. The default is %s. If you are running inference with the cascade and the folder '\n",
    "                             'pointed to by --lowres_segmentations does not contain the segmentation maps generated by '\n",
    "                             'the low resolution U-Net then the low resolution segmentation maps will be automatically '\n",
    "                             'generated. For this case, make sure to set the trainer class here that matches your '\n",
    "                             '--cascade_trainer_class_name (this part can be ignored if defaults are used).'\n",
    "                             % default_trainer,\n",
    "                        required=False,\n",
    "                        default=default_trainer)\n",
    "    parser.add_argument('-ctr', '--cascade_trainer_class_name',\n",
    "                        help=\"Trainer class name used for predicting the 3D full resolution U-Net part of the cascade.\"\n",
    "                             \"Default is %s\" % default_cascade_trainer, required=False,\n",
    "                        default=default_cascade_trainer)\n",
    "\n",
    "    parser.add_argument('-m', '--model', help=\"2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres. Default: 3d_fullres\",\n",
    "                        default=\"3d_fullres\", required=False)\n",
    "\n",
    "    parser.add_argument('-p', '--plans_identifier', help='do not touch this unless you know what you are doing',\n",
    "                        default=default_plans_identifier, required=False)\n",
    "\n",
    "    parser.add_argument('-f', '--folds', nargs='+', default='None',\n",
    "                        help=\"folds to use for prediction. Default is None which means that folds will be detected \"\n",
    "                             \"automatically in the model output folder\")\n",
    "\n",
    "    parser.add_argument('-z', '--save_npz', required=False, action='store_true',\n",
    "                        help=\"use this if you want to ensemble these predictions with those of other models. Softmax \"\n",
    "                             \"probabilities will be saved as compressed numpy arrays in output_folder and can be \"\n",
    "                             \"merged between output_folders with nnUNet_ensemble_predictions\")\n",
    "\n",
    "    parser.add_argument('-l', '--lowres_segmentations', required=False, default='None',\n",
    "                        help=\"if model is the highres stage of the cascade then you can use this folder to provide \"\n",
    "                             \"predictions from the low resolution 3D U-Net. If this is left at default, the \"\n",
    "                             \"predictions will be generated automatically (provided that the 3D low resolution U-Net \"\n",
    "                             \"network weights are present\")\n",
    "\n",
    "    parser.add_argument(\"--part_id\", type=int, required=False, default=0, help=\"Used to parallelize the prediction of \"\n",
    "                                                                               \"the folder over several GPUs. If you \"\n",
    "                                                                               \"want to use n GPUs to predict this \"\n",
    "                                                                               \"folder you need to run this command \"\n",
    "                                                                               \"n times with --part_id=0, ... n-1 and \"\n",
    "                                                                               \"--num_parts=n (each with a different \"\n",
    "                                                                               \"GPU (for example via \"\n",
    "                                                                               \"CUDA_VISIBLE_DEVICES=X)\")\n",
    "\n",
    "    parser.add_argument(\"--num_parts\", type=int, required=False, default=1,\n",
    "                        help=\"Used to parallelize the prediction of \"\n",
    "                             \"the folder over several GPUs. If you \"\n",
    "                             \"want to use n GPUs to predict this \"\n",
    "                             \"folder you need to run this command \"\n",
    "                             \"n times with --part_id=0, ... n-1 and \"\n",
    "                             \"--num_parts=n (each with a different \"\n",
    "                             \"GPU (via \"\n",
    "                             \"CUDA_VISIBLE_DEVICES=X)\")\n",
    "\n",
    "    parser.add_argument(\"--num_threads_preprocessing\", required=False, default=6, type=int, help=\n",
    "    \"Determines many background processes will be used for data preprocessing. Reduce this if you \"\n",
    "    \"run into out of memory (RAM) problems. Default: 6\")\n",
    "\n",
    "    parser.add_argument(\"--num_threads_nifti_save\", required=False, default=2, type=int, help=\n",
    "    \"Determines many background processes will be used for segmentation export. Reduce this if you \"\n",
    "    \"run into out of memory (RAM) problems. Default: 2\")\n",
    "\n",
    "    parser.add_argument(\"--disable_tta\", required=False, default=False, action=\"store_true\",\n",
    "                        help=\"set this flag to disable test time data augmentation via mirroring. Speeds up inference \"\n",
    "                             \"by roughly factor 4 (2D) or 8 (3D)\")\n",
    "\n",
    "    parser.add_argument(\"--overwrite_existing\", required=False, default=False, action=\"store_true\",\n",
    "                        help=\"Set this flag if the target folder contains predictions that you would like to overwrite\")\n",
    "\n",
    "    parser.add_argument(\"--mode\", type=str, default=\"normal\", required=False, help=\"Hands off!\")\n",
    "    parser.add_argument(\"--all_in_gpu\", type=str, default=\"None\", required=False, help=\"can be None, False or True. \"\n",
    "                                                                                       \"Do not touch.\")\n",
    "    parser.add_argument(\"--step_size\", type=float, default=0.5, required=False, help=\"don't touch\")\n",
    "    # parser.add_argument(\"--interp_order\", required=False, default=3, type=int,\n",
    "    #                     help=\"order of interpolation for segmentations, has no effect if mode=fastest. Do not touch this.\")\n",
    "    # parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,\n",
    "    #                     help=\"order of interpolation along z is z is done differently. Do not touch this.\")\n",
    "    # parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,\n",
    "    #                     help=\"force_separate_z resampling. Can be None, True or False, has no effect if mode=fastest. \"\n",
    "    #                          \"Do not touch this.\")\n",
    "    parser.add_argument('-chk',\n",
    "                        help='checkpoint name, default: model_final_checkpoint',\n",
    "                        required=False,\n",
    "                        default='model_final_checkpoint')\n",
    "    parser.add_argument('--disable_mixed_precision', default=False, action='store_true', required=False,\n",
    "                        help='Predictions are done with mixed precision by default. This improves speed and reduces '\n",
    "                             'the required vram. If you want to disable mixed precision you can set this flag. Note '\n",
    "                             'that yhis is not recommended (mixed precision is ~2x faster!)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    input_folder = args.input_folder\n",
    "    output_folder = args.output_folder\n",
    "    part_id = args.part_id\n",
    "    num_parts = args.num_parts\n",
    "    folds = args.folds\n",
    "    save_npz = args.save_npz\n",
    "    lowres_segmentations = args.lowres_segmentations\n",
    "    num_threads_preprocessing = args.num_threads_preprocessing\n",
    "    num_threads_nifti_save = args.num_threads_nifti_save\n",
    "    disable_tta = args.disable_tta\n",
    "    step_size = args.step_size\n",
    "    # interp_order = args.interp_order\n",
    "    # interp_order_z = args.interp_order_z\n",
    "    # force_separate_z = args.force_separate_z\n",
    "    overwrite_existing = args.overwrite_existing\n",
    "    mode = args.mode\n",
    "    all_in_gpu = args.all_in_gpu\n",
    "    model = args.model\n",
    "    trainer_class_name = args.trainer_class_name\n",
    "    cascade_trainer_class_name = args.cascade_trainer_class_name\n",
    "\n",
    "    task_name = args.task_name\n",
    "\n",
    "    if not task_name.startswith(\"Task\"):\n",
    "        task_id = int(task_name)\n",
    "        task_name = convert_id_to_task_name(task_id)\n",
    "\n",
    "    assert model in [\"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"], \"-m must be 2d, 3d_lowres, 3d_fullres or \" \\\n",
    "                                                                             \"3d_cascade_fullres\"\n",
    "\n",
    "    # if force_separate_z == \"None\":\n",
    "    #     force_separate_z = None\n",
    "    # elif force_separate_z == \"False\":\n",
    "    #     force_separate_z = False\n",
    "    # elif force_separate_z == \"True\":\n",
    "    #     force_separate_z = True\n",
    "    # else:\n",
    "    #     raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)\n",
    "\n",
    "    if lowres_segmentations == \"None\":\n",
    "        lowres_segmentations = None\n",
    "\n",
    "    if isinstance(folds, list):\n",
    "        if folds[0] == 'all' and len(folds) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            folds = [int(i) for i in folds]\n",
    "    elif folds == \"None\":\n",
    "        folds = None\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected value for argument folds\")\n",
    "\n",
    "    assert all_in_gpu in ['None', 'False', 'True']\n",
    "    if all_in_gpu == \"None\":\n",
    "        all_in_gpu = None\n",
    "    elif all_in_gpu == \"True\":\n",
    "        all_in_gpu = True\n",
    "    elif all_in_gpu == \"False\":\n",
    "        all_in_gpu = False\n",
    "\n",
    "    # we need to catch the case where model is 3d cascade fullres and the low resolution folder has not been set.\n",
    "    # In that case we need to try and predict with 3d low res first\n",
    "    if model == \"3d_cascade_fullres\" and lowres_segmentations is None:\n",
    "        print(\"lowres_segmentations is None. Attempting to predict 3d_lowres first...\")\n",
    "        assert part_id == 0 and num_parts == 1, \"if you don't specify a --lowres_segmentations folder for the \" \\\n",
    "                                                \"inference of the cascade, custom values for part_id and num_parts \" \\\n",
    "                                                \"are not supported. If you wish to have multiple parts, please \" \\\n",
    "                                                \"run the 3d_lowres inference first (separately)\"\n",
    "        model_folder_name = join(network_training_output_dir, \"3d_lowres\", task_name, trainer_class_name + \"__\" +\n",
    "                                  args.plans_identifier)\n",
    "        assert isdir(model_folder_name), \"model output folder not found. Expected: %s\" % model_folder_name\n",
    "        lowres_output_folder = join(output_folder, \"3d_lowres_predictions\")\n",
    "        predict_from_folder(model_folder_name, input_folder, lowres_output_folder, folds, False,\n",
    "                            num_threads_preprocessing, num_threads_nifti_save, None, part_id, num_parts, not disable_tta,\n",
    "                            overwrite_existing=overwrite_existing, mode=mode, overwrite_all_in_gpu=all_in_gpu,\n",
    "                            mixed_precision=not args.disable_mixed_precision,\n",
    "                            step_size=step_size)\n",
    "        lowres_segmentations = lowres_output_folder\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"3d_lowres done\")\n",
    "\n",
    "    if model == \"3d_cascade_fullres\":\n",
    "        trainer = cascade_trainer_class_name\n",
    "    else:\n",
    "        trainer = trainer_class_name\n",
    "\n",
    "    model_folder_name = join(network_training_output_dir, model, task_name, trainer + \"__\" +\n",
    "                              args.plans_identifier)\n",
    "    print(\"using model stored in \", model_folder_name)\n",
    "    assert isdir(model_folder_name), \"model output folder not found. Expected: %s\" % model_folder_name\n",
    "\n",
    "    predict_from_folder(model_folder_name, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,\n",
    "                        num_threads_nifti_save, lowres_segmentations, part_id, num_parts, not disable_tta,\n",
    "                        overwrite_existing=overwrite_existing, mode=mode, overwrite_all_in_gpu=all_in_gpu,\n",
    "                        mixed_precision=not args.disable_mixed_precision,\n",
    "                        step_size=step_size, checkpoint_name=args.chk)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresult\\n\\nmaybe_case_ids = array(['23010017_20141226', '23010018_20141226', '23010019_20141224'], dtype='<U17')\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities):\n",
    "    print(\"This model expects %d input modalities for each image\" % expected_num_modalities)\n",
    "    files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
    "\n",
    "    maybe_case_ids = np.unique([i[:-12] for i in files])\n",
    "\n",
    "    remaining = deepcopy(files)\n",
    "    missing = []\n",
    "\n",
    "    assert len(files) > 0, \"input folder did not contain any images (expected to find .nii.gz file endings)\"\n",
    "\n",
    "    # now check if all required files are present and that no unexpected files are remaining\n",
    "    for c in maybe_case_ids:\n",
    "        for n in range(expected_num_modalities):\n",
    "            expected_output_file = c + \"_%04.0d.nii.gz\" % n\n",
    "            if not isfile(join(input_folder, expected_output_file)):\n",
    "                missing.append(expected_output_file)\n",
    "            else:\n",
    "                remaining.remove(expected_output_file)\n",
    "\n",
    "    print(\"Found %d unique case ids, here are some examples:\" % len(maybe_case_ids),\n",
    "          np.random.choice(maybe_case_ids, min(len(maybe_case_ids), 10)))\n",
    "    print(\"If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\")\n",
    "\n",
    "    if len(remaining) > 0:\n",
    "        print(\"found %d unexpected remaining files in the folder. Here are some examples:\" % len(remaining),\n",
    "              np.random.choice(remaining, min(len(remaining), 10)))\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        print(\"Some files are missing:\")\n",
    "        print(missing)\n",
    "        raise RuntimeError(\"missing files in input_folder\")\n",
    "\n",
    "    return maybe_case_ids\n",
    "\n",
    "'''\n",
    "result\n",
    "\n",
    "maybe_case_ids = array(['23010017_20141226', '23010018_20141226', '23010019_20141224'], dtype='<U17')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_folder(model: str, input_folder: str, output_folder: str, folds: Union[Tuple[int], List[int]],\n",
    "                        save_npz: bool, num_threads_preprocessing: int, num_threads_nifti_save: int,\n",
    "                        lowres_segmentations: Union[str, None],\n",
    "                        part_id: int, num_parts: int, tta: bool, mixed_precision: bool = True,\n",
    "                        overwrite_existing: bool = True,\n",
    "                        step_size: float = 0.5, checkpoint_name: str = \"model_final_checkpoint\",\n",
    "                        segmentation_export_kwargs: dict = None, disable_postprocessing: bool = False):\n",
    "    \"\"\"\n",
    "        here we use the standard naming scheme to generate list_of_lists and output_files needed by predict_cases\n",
    "    :param model:\n",
    "    :param input_folder:\n",
    "    :param output_folder:\n",
    "    :param folds:\n",
    "    :param save_npz:\n",
    "    :param num_threads_preprocessing:\n",
    "    :param num_threads_nifti_save:\n",
    "    :param lowres_segmentations:\n",
    "    :param part_id:\n",
    "    :param num_parts:\n",
    "    :param tta:\n",
    "    :param mixed_precision:\n",
    "    :param overwrite_existing: if not None then it will be overwritten with whatever is in there. None is default (no overwrite)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    maybe_mkdir_p(output_folder)\n",
    "    shutil.copy(join(model, 'plans.pkl'), output_folder)\n",
    "\n",
    "    assert isfile(join(model, \"plans.pkl\")), \"Folder with saved model weights must contain a plans.pkl file\"\n",
    "    expected_num_modalities = load_pickle(join(model, \"plans.pkl\"))['num_modalities']\n",
    "\n",
    "    # check input folder integrity # case id (equal count)\n",
    "    case_ids = check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities)\n",
    "    case_ids = \n",
    "\n",
    "    output_files = [join(output_folder, i + \".nii.gz\") for i in case_ids]\n",
    "    all_files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
    "    list_of_lists = [[join(input_folder, i) for i in all_files if i[:len(j)].startswith(j) and\n",
    "                      len(i) == (len(j) + 12)] for j in case_ids]\n",
    "\n",
    "\n",
    "\n",
    "    return predict_cases(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
    "                         save_npz, num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations, tta,\n",
    "                         mixed_precision=mixed_precision, overwrite_existing=overwrite_existing,\n",
    "                         all_in_gpu=all_in_gpu,\n",
    "                         step_size=step_size, checkpoint_name=checkpoint_name,\n",
    "                         segmentation_export_kwargs=segmentation_export_kwargs,\n",
    "                         disable_postprocessing=disable_postprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_case_ids_ = np.unique([i[:-12] for i in sub_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./input/23010017_20141226', './input/23010018_20141226',\n",
       "       './input/23010019_20141224'], dtype='<U25')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_case_ids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input/23010017_20141226_0000.nii.gz',\n",
       " './input/23010017_20141226_0001.nii.gz',\n",
       " './input/23010018_20141226_0000.nii.gz',\n",
       " './input/23010018_20141226_0001.nii.gz',\n",
       " './input/23010019_20141224_0000.nii.gz',\n",
       " './input/23010019_20141224_0001.nii.gz']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ = subfiles('./input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = './model'\n",
    "list_of_lists_ = os.listdir('./input')\n",
    "num_threads_preprocessing_ = 6\n",
    "num_threads_nifti_save_ = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cases(model, list_of_lists, output_filenames, folds, save_npz=False, num_threads_preprocessing,\n",
    "                  num_threads_nifti_save, segs_from_prev_stage=None, do_tta=False, mixed_precision=True,\n",
    "                  all_in_gpu=False, step_size=0.5, checkpoint_name=\"model_final_checkpoint\",\n",
    "                  segmentation_export_kwargs: dict = None, disable_postprocessing: bool = False):\n",
    "    \"\"\"\n",
    "    :param segmentation_export_kwargs:\n",
    "    :param model: folder where the model is saved, must contain fold_x subfolders\n",
    "    :param list_of_lists: [[case0_0000.nii.gz, case0_0001.nii.gz], [case1_0000.nii.gz, case1_0001.nii.gz], ...]\n",
    "    :param output_filenames: [output_file_case0.nii.gz, output_file_case1.nii.gz, ...]\n",
    "    :param folds: default: (0, 1, 2, 3, 4) (but can also be 'all' or a subset of the five folds, for example use (0, )\n",
    "    for using only fold_0\n",
    "    :param save_npz: default: False\n",
    "    :param num_threads_preprocessing:\n",
    "    :param num_threads_nifti_save:\n",
    "    :param segs_from_prev_stage:\n",
    "    :param do_tta: default: True, can be set to False for a 8x speedup at the cost of a reduced segmentation quality\n",
    "    :param overwrite_existing: default: True\n",
    "    :param mixed_precision: if None then we take no action. If True/False we overwrite what the model has in its init\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(list_of_lists) == len(output_filenames)\n",
    "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
    "\n",
    "    pool = Pool(num_threads_nifti_save)\n",
    "    results = []\n",
    "\n",
    "    cleaned_output_files = []\n",
    "    for o in output_filenames:\n",
    "        dr, f = os.path.split(o)\n",
    "        if len(dr) > 0:\n",
    "            maybe_mkdir_p(dr)\n",
    "        if not f.endswith(\".nii.gz\"):\n",
    "            f, _ = os.path.splitext(f)\n",
    "            f = f + \".nii.gz\"\n",
    "        cleaned_output_files.append(join(dr, f))\n",
    "\n",
    "    if not overwrite_existing:\n",
    "        print(\"number of cases:\", len(list_of_lists))\n",
    "        # if save_npz=True then we should also check for missing npz files\n",
    "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if (not isfile(j)) or (save_npz and not isfile(j[:-7] + '.npz'))]\n",
    "\n",
    "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
    "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
    "        if segs_from_prev_stage is not None:\n",
    "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
    "\n",
    "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
    "\n",
    "    print(\"emptying cuda cache\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"loading parameters for folds,\", folds)\n",
    "    trainer, params = load_model_and_checkpoint_files(model, folds, mixed_precision=mixed_precision,\n",
    "                                                      checkpoint_name=checkpoint_name)\n",
    "\n",
    "    if segmentation_export_kwargs is None:\n",
    "        if 'segmentation_export_params' in trainer.plans.keys():\n",
    "            force_separate_z = trainer.plans['segmentation_export_params']['force_separate_z']\n",
    "            interpolation_order = trainer.plans['segmentation_export_params']['interpolation_order']\n",
    "            interpolation_order_z = trainer.plans['segmentation_export_params']['interpolation_order_z']\n",
    "        else:\n",
    "            force_separate_z = None\n",
    "            interpolation_order = 1\n",
    "            interpolation_order_z = 0\n",
    "    else:\n",
    "        force_separate_z = segmentation_export_kwargs['force_separate_z']\n",
    "        interpolation_order = segmentation_export_kwargs['interpolation_order']\n",
    "        interpolation_order_z = segmentation_export_kwargs['interpolation_order_z']\n",
    "\n",
    "    print(\"starting preprocessing generator\")\n",
    "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
    "                                             segs_from_prev_stage)\n",
    "    print(\"starting prediction...\")\n",
    "    all_output_files = []\n",
    "    for preprocessed in preprocessing:\n",
    "        output_filename, (d, dct) = preprocessed\n",
    "        all_output_files.append(all_output_files)\n",
    "        if isinstance(d, str):\n",
    "            data = np.load(d)\n",
    "            os.remove(d)\n",
    "            d = data\n",
    "\n",
    "        print(\"predicting\", output_filename)\n",
    "        trainer.load_checkpoint_ram(params[0], False)\n",
    "        softmax = trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
    "            d, do_mirroring=do_tta, mirror_axes=trainer.data_aug_params['mirror_axes'], use_sliding_window=True,\n",
    "            step_size=step_size, use_gaussian=True, all_in_gpu=all_in_gpu,\n",
    "            mixed_precision=mixed_precision)[1]\n",
    "\n",
    "        for p in params[1:]:\n",
    "            trainer.load_checkpoint_ram(p, False)\n",
    "            softmax += trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
    "                d, do_mirroring=do_tta, mirror_axes=trainer.data_aug_params['mirror_axes'], use_sliding_window=True,\n",
    "                step_size=step_size, use_gaussian=True, all_in_gpu=all_in_gpu,\n",
    "                mixed_precision=mixed_precision)[1]\n",
    "\n",
    "        if len(params) > 1:\n",
    "            softmax /= len(params)\n",
    "\n",
    "        transpose_forward = trainer.plans.get('transpose_forward')\n",
    "        if transpose_forward is not None:\n",
    "            transpose_backward = trainer.plans.get('transpose_backward')\n",
    "            softmax = softmax.transpose([0] + [i + 1 for i in transpose_backward])\n",
    "\n",
    "        if save_npz:\n",
    "            npz_file = output_filename[:-7] + \".npz\"\n",
    "        else:\n",
    "            npz_file = None\n",
    "\n",
    "        if hasattr(trainer, 'regions_class_order'):\n",
    "            region_class_order = trainer.regions_class_order\n",
    "        else:\n",
    "            region_class_order = None\n",
    "\n",
    "        \"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n",
    "        larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n",
    "        communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n",
    "        enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n",
    "        patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n",
    "        then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n",
    "        filename or np.ndarray and will handle this automatically\"\"\"\n",
    "        bytes_per_voxel = 4\n",
    "        if all_in_gpu:\n",
    "            bytes_per_voxel = 2  # if all_in_gpu then the return value is half (float16)\n",
    "        if np.prod(softmax.shape) > (2e9 / bytes_per_voxel * 0.85):  # * 0.85 just to be save\n",
    "            print(\n",
    "                \"This output is too large for python process-process communication. Saving output temporarily to disk\")\n",
    "            np.save(output_filename[:-7] + \".npy\", softmax)\n",
    "            softmax = output_filename[:-7] + \".npy\"\n",
    "\n",
    "        results.append(pool.starmap_async(save_segmentation_nifti_from_softmax,\n",
    "                                          ((softmax, output_filename, dct, interpolation_order, region_class_order,\n",
    "                                            None, None,\n",
    "                                            npz_file, None, force_separate_z, interpolation_order_z),)\n",
    "                                          ))\n",
    "\n",
    "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
    "    _ = [i.get() for i in results]\n",
    "    # now apply postprocessing\n",
    "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
    "    if not disable_postprocessing:\n",
    "        results = []\n",
    "        pp_file = join(model, \"postprocessing.json\")\n",
    "        if isfile(pp_file):\n",
    "            print(\"postprocessing...\")\n",
    "            shutil.copy(pp_file, os.path.abspath(os.path.dirname(output_filenames[0])))\n",
    "            # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
    "            # removed\n",
    "            for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
    "            results.append(pool.starmap_async(load_remove_save,\n",
    "                                              zip(output_filenames, output_filenames,\n",
    "                                                  [for_which_classes] * len(output_filenames),\n",
    "                                                  [min_valid_obj_size] * len(output_filenames))))\n",
    "            _ = [i.get() for i in results]\n",
    "        else:\n",
    "            print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
    "                  \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
    "                  \"%s\" % model)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
