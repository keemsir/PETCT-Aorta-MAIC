{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Obtaining file:///mnt/submission/submit_files/nnUNet\n",
      "Requirement already satisfied: torch>=1.6.0a in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.9.1+cu111)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (4.62.3)\n",
      "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2.3.0)\n",
      "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.18.3)\n",
      "Requirement already satisfied: medpy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.4.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.7.1)\n",
      "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.23)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.19.5)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.0)\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2.1.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.3.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from nnunet==1.7.0) (2.22.0)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (3.2.1)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2021.10.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0a->nnunet==1.7.0) (3.7.4.3)\n",
      "Requirement already satisfied: pydicom>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from dicom2nifti->nnunet==1.7.0) (2.2.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.6.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (3.4.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (8.3.2)\n",
      "Requirement already satisfied: unittest2 in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nnunet==1.7.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nnunet==1.7.0) (2021.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from nibabel->nnunet==1.7.0) (21.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (2.4.7)\n",
      "Requirement already satisfied: argparse in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: traceback2 in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: linecache2 in /usr/local/lib/python3.8/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)\n",
      "Installing collected packages: nnunet\n",
      "  Attempting uninstall: nnunet\n",
      "    Found existing installation: nnunet 1.7.0\n",
      "    Can't uninstall 'nnunet'. No files were found to uninstall.\n",
      "  Running setup.py develop for nnunet\n",
      "Successfully installed nnunet\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def maybe_mkdir_p(directory: str) -> None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# maic_dir = '/mnt/backup/'\n",
    "# base_dir = os.path.join(maic_dir, 'working')\n",
    "base_dir = os.getcwd()\n",
    "input_dir = '/mnt/NM/dataset/'\n",
    "temp_dir = os.path.join(base_dir, 'convert_data')\n",
    "\n",
    "# maybe_mkdir_p(base_dir)\n",
    "maybe_mkdir_p(temp_dir)\n",
    "\n",
    "# ! git clone https://github.com/keemsir/nnUNet.git\n",
    "\n",
    "respository_dir = os.path.join(base_dir, 'models/module')\n",
    "os.chdir(respository_dir)\n",
    "\n",
    "! pip install -e .\n",
    "\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import special\n",
    "import copy\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# must install\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Completed!\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Task55_PETCT'\n",
    "convert_name = 'Task555_PETCT'\n",
    "\n",
    "main_dir = os.path.join(base_dir, 'nnUNet/nnunet')\n",
    "mainT_dir = os.path.join(temp_dir, 'nnUNet/nnunet')\n",
    "\n",
    "rawbase_dir = os.path.join(mainT_dir, 'nnUNet_raw_data_base/')\n",
    "\n",
    "pp_dir = os.path.join(mainT_dir, 'preprocessed')\n",
    "tasks_dir = os.path.join(mainT_dir, 'Tasks')\n",
    "task_dir = os.path.join(tasks_dir, task_name)\n",
    "\n",
    "model_dir = os.path.join(main_dir, 'nnUNet_trained_models')\n",
    "Prediction_dir = os.path.join(main_dir, 'nnUNet_Prediction_Results')\n",
    "result_dir = os.path.join(Prediction_dir, convert_name)\n",
    "\n",
    "staple_dir = os.path.join(Prediction_dir, 'staple')\n",
    "\n",
    "# 1. Data preprocessing\n",
    "maybe_mkdir_p(tasks_dir)\n",
    "maybe_mkdir_p(temp_dir)\n",
    "\n",
    "# 2. Directory\n",
    "maybe_mkdir_p(main_dir)\n",
    "maybe_mkdir_p(model_dir)\n",
    "maybe_mkdir_p(pp_dir)\n",
    "\n",
    "# 3. Directory\n",
    "maybe_mkdir_p(result_dir)\n",
    "maybe_mkdir_p(staple_dir)\n",
    "\n",
    "\n",
    "#Environment Setting\n",
    "os.environ['nnUNet_raw_data_base'] = rawbase_dir #os.path.join(mainT_dir, 'nnUNet_raw_data_base')\n",
    "os.environ['nnUNet_preprocessed'] = pp_dir #os.path.join(mainT_dir, 'preprocessed')\n",
    "os.environ['RESULTS_FOLDER'] = './models' #os.path.join(main_dir, 'nnUNet_trained_models')\n",
    "\n",
    "print('Setting Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating \"Task55_PETCT\" Image & Label ..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/NM/dataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2c35373783bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image Patient : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mhdf2nifti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2c35373783bb>\u001b[0m in \u001b[0;36mhdf2nifti\u001b[0;34m(hdf_folder, save_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmaybe_mkdir_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'imagesTs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating \"{}\" Image & Label ..'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhdf5_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/NM/dataset/'"
     ]
    }
   ],
   "source": [
    "def hdf2nifti(hdf_folder: str, save_folder: str):\n",
    "    # hdf_folder : [train_dir, test_dir] hdf5 file path\n",
    "    # save_folder : [imagesTr, imagesTs] Save Folder path\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'imagesTr'))\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'imagesTs'))\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'labelsTr'))\n",
    "    print('Creating \"{}\" Image & Label ..'.format(os.path.basename(os.path.normpath(save_folder))))\n",
    "    hdf5_files = os.listdir(hdf_folder)\n",
    "\n",
    "\n",
    "    for hdf5_file in hdf5_files:\n",
    "\n",
    "\n",
    "        hdf5_path = os.path.join(hdf_folder, hdf5_file)\n",
    "\n",
    "        # image\n",
    "        f_i = h5py.File(hdf5_path, 'r')\n",
    "        ctarr = np.asarray(f_i['CT'])\n",
    "        petarr = np.asarray(f_i['PET'])\n",
    "        f_i.close()\n",
    "\n",
    "        SLICE_SIZE_X, SLICE_SIZE_Y, SLICE_COUNT = ctarr.shape\n",
    "        images = np.empty([SLICE_SIZE_X, SLICE_SIZE_Y, SLICE_COUNT, 0], dtype=np.single)\n",
    "\n",
    "        image_ct = np.expand_dims(ctarr, axis=3)\n",
    "        images = np.append(images, image_ct, axis=3)\n",
    "        image_pet = np.expand_dims(petarr, axis=3)\n",
    "        images = np.append(images, image_pet, axis=3)\n",
    "\n",
    "\n",
    "        hdf5_file_NAME = hdf5_file\n",
    "\n",
    "        niim = nib.Nifti1Image(images, affine=np.eye(4))\n",
    "        nib.save(niim, os.path.join(save_folder, 'imagesTs/{}.nii.gz'.format(hdf5_file[:-8])))\n",
    "\n",
    "\n",
    "    print('\"{}\" Image & Label Completed !!'.format(os.path.basename(os.path.normpath(save_folder))))\n",
    "    print('Image Patient : {}'.format(len(os.listdir(input_dir))))\n",
    "\n",
    "hdf2nifti(input_dir, task_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/temp/nnUNet/nnunet/Tasks/Task55_PETCT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-104991aedf2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/temp/nnUNet/nnunet/Tasks/Task55_PETCT'"
     ]
    }
   ],
   "source": [
    "def json_mk(save_dir: str):\n",
    "    # Path\n",
    "    imagesTr = os.path.join(save_dir, 'imagesTr')\n",
    "    imagesTs = os.path.join(save_dir, 'imagesTs')\n",
    "    maybe_mkdir_p(imagesTr)\n",
    "    maybe_mkdir_p(imagesTs)\n",
    "\n",
    "    overwrite_json_file = True\n",
    "    json_file_exist = False\n",
    "\n",
    "    if os.path.exists(os.path.join(save_dir, 'dataset.json')):\n",
    "        print('dataset.json already exist!')\n",
    "        json_file_exist = True\n",
    "\n",
    "    if json_file_exist == False or overwrite_json_file:\n",
    "\n",
    "        json_dict = OrderedDict()\n",
    "        json_dict['name'] = \"PETCT\"\n",
    "        json_dict['description'] = \"Medical Image AI Challenge 2021\"\n",
    "        json_dict['tensorImageSize'] = \"4D\"\n",
    "        json_dict['reference'] = \"https://maic.or.kr/competitions/\"\n",
    "        json_dict['licence'] = \"SNUH\"\n",
    "        json_dict['release'] = \"18/10/2021\"\n",
    "\n",
    "        json_dict['modality'] = {\n",
    "            \"0\": \"CT\",\n",
    "            \"1\": \"PET\"\n",
    "        }\n",
    "        json_dict['labels'] = {\n",
    "            \"0\": \"background\",\n",
    "            \"1\": \"Aorta\"\n",
    "        }\n",
    "\n",
    "        train_ids = sorted(os.listdir(imagesTr))\n",
    "        test_ids = sorted(os.listdir(imagesTs))\n",
    "        json_dict['numTraining'] = len(train_ids)\n",
    "        json_dict['numTest'] = len(test_ids)\n",
    "\n",
    "        json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "        json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids] #(i[:i.find(\"_0000\")])\n",
    "\n",
    "        with open(os.path.join(save_dir, \"dataset.json\"), 'w') as f:\n",
    "            json.dump(json_dict, f, indent=4, sort_keys=False)\n",
    "\n",
    "        if os.path.exists(os.path.join(save_dir, 'dataset.json')):\n",
    "            if json_file_exist == False:\n",
    "                print('dataset.json created!')\n",
    "            else:\n",
    "                print('dataset.json overwritten!')\n",
    "json_mk(task_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!nnUNet_convert_decathlon_task -i ./convert_data/nnUNet/nnunet/Tasks/Task55_PETCT -output_task_id 555 # -i : task_dir\n",
    "# !nnUNet_plan_and_preprocess -t 555 # --verify_dataset_integrity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "using model stored in  ./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1\n",
      "This model expects 2 input modalities for each image\n",
      "Found 1 unique case ids, here are some examples: ['23010018_20141226']\n",
      "If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\n",
      "number of cases: 1\n",
      "number of cases that still need to be predicted: 1\n",
      "emptying cuda cache\n",
      "loading parameters for folds, None\n",
      "folds is None so we will automatically look for output folders (not using 'all'!)\n",
      "found the following folds:  ['./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4']\n",
      "using the following model files:  ['./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']\n",
      "starting preprocessing generator\n",
      "starting prediction...\n",
      "preprocessing ./23010018_20141226.nii.gz\n",
      "using preprocessor PreprocessorFor2D\n",
      "before crop: (2, 284, 200, 200) after crop: (2, 284, 173, 173) spacing: [1. 1. 1.] \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (2, 284, 173, 173)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (2, 284, 173, 173)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "(2, 284, 173, 173)\n",
      "This worker has ended successfully, no errors to report\n",
      "predicting ./23010018_20141226.nii.gz\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "inference done. Now waiting for the segmentation export to finish...\n",
      "force_separate_z: None interpolation order: 1\n",
      "no resampling necessary\n",
      "WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run consolidate_folds in the output folder of the model first!\n",
      "The folder you need to run this in is ./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1\n"
     ]
    }
   ],
   "source": [
    "!nnUNet_predict -i ./convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs -o ./result -t 555 -tr nnUNetTrainerV2 -m 2d --disable_tta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suv_params(ptarr, roi):\n",
    "    roi = np.asarray(roi>0, dtype=np.float)\n",
    "    suvmax = np.max(ptarr*roi)\n",
    "    suvmean = np.sum(ptarr*roi)/np.sum(roi)\n",
    "    return suvmax, suvmean\n",
    "\n",
    "def get_vol_params(ptzoom, roi):\n",
    "    roi = np.asarray(roi>0, dtype=np.float)\n",
    "    return np.prod(ptzoom) * np.sum(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_list = os.listdir('./result')\n",
    "\n",
    "for pn_l in PN_list:\n",
    "    _, ext = os.path.splitext(pn_l)\n",
    "    if ext == '.gz':\n",
    "        \n",
    "        pn = pn_l[:-7]\n",
    "\n",
    "        ptarr = np.array(nib.load('./convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs/{}_0001.nii.gz'.format(pn)).dataobj)\n",
    "        ctarr = np.array(nib.load('./convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs/{}_0000.nii.gz'.format(pn)).dataobj)\n",
    "        pred_arr = np.array(nib.load('./result/{}.nii.gz'.format(pn)).dataobj)\n",
    "\n",
    "        size = ([4.07283, 4.07283, 3.])\n",
    "\n",
    "        #Calculate Mean SUV and Max SUV\n",
    "\n",
    "        suvmax, suvmean = get_suv_params(ptarr, pred_arr)\n",
    "\n",
    "        #Calculate Volume\n",
    "\n",
    "        aorvol = get_vol_params(size, pred_arr)\n",
    "        \n",
    "        PATIENT_NUM = pn\n",
    "\n",
    "        data = {'case' : [PATIENT_NUM], 'PD_Aorta_volume' : [aorvol], 'PD_SUVmean' : [suvmean]}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # .to_csv \n",
    "        if not os.path.exists('submission.csv'):\n",
    "            df.to_csv('submission.csv', index=False, mode='w')\n",
    "        else:\n",
    "            df.to_csv('submission.csv', index=False, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2 0.3]\n",
      " [0.4 0.5 0.6]]\n",
      "[[0 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "num_list = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "print(num_list)\n",
    "numm = np.uint8((num_list >= 0.3))\n",
    "print(numm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1l\u001b> 1 root       20   0 \u001b[36m 6\u001b[m\u001b[m004 \u001b[36m 3\u001b[m\u001b[m728 \u001b[36m 3\u001b[m\u001b[m248 S  0.0  0.0  0:00.02 /bin/bash /root/e\u001b[1;1H   0 \u001b[36m 547M 51\u001b[m\u001b[m520 \u001b[36m14\u001b[m\u001b[m416 S  0.7  0.0  0:01.81 /usr/bin/python3h /root/e\u001b[20;4H\u001b[m\u001b[m14 root\u001b[7C20   0 \u001b[36m 293M  107M 18\u001b[m\u001b[m808 S  0.0  0.1  0:00.00 \u001b[32m/usr/bin/python3 \u001b[21;4H\u001b[m\u001b[m15 root\u001b[7C20   0 \u001b[36m 293M  107M 18\u001b[m\u001b[m808 S  0.0  0.1  0:00.07 \u001b[32m/usr/bin/python3 \u001b[22;5H\u001b[m\u001b[m8 root\u001b[7C20   0 \u001b[36m 293M  107M 18\u001b[m\u001b[m808 S  0.0  0.1  1:27.03 /usr/bin/python3\u001b[23;4H20 root\u001b[7C20   0 \u001b[36m 547M 51\u001b[m\u001b[m520 \u001b[36m14\u001b[m\u001b[m416 S  0.0  0.0  0:00.00 \u001b[32m/usr/bin/python3 \u001b[24;1H\u001b[m\u001b[mF1\u001b[30m\u001b[46mHelp  \u001b[m\u001b[mF2\u001b[30m\u001b[46mSetup \u001b[m\u001b[mF3\u001b[30m\u001b[46mSearch\u001b[m\u001b[mF4\u001b[30m\u001b[46mFilter\u001b[m\u001b[mF5\u001b[30m\u001b[46mTree  \u001b[m\u001b[mF6\u001b[30m\u001b[46mSortBy\u001b[m\u001b[mF7\u001b[30m\u001b[46mNice -\u001b[m\u001b[mF8\u001b[30m\u001b[46mNice +\u001b[m\u001b[mF9\u001b[30m\u001b[46mKill  \u001b[m\u001b[mF10\u001b[30m\u001b[46mQui\u001b[4ht\u001b[4l\u001b[H\u001b[m\u001b[m[\u001b[30m       0.0%\u001b[m]\u001b[7;3H\u001b[m\u001b[36m6  \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m18 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m30 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m42 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[8;3H\u001b[m\u001b[36m7  \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m19 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m31 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m43 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[9;3H\u001b[m\u001b[36m8  \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m20 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m32 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m44 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[10;3H\u001b[m\u001b[36m9  \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m21 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m33 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m45 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[11;3H\u001b[m\u001b[36m10 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m22 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m34 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m46 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[12;3H\u001b[m\u001b[36m11 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m23 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m35 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m47 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[13;3H\u001b[m\u001b[36m12 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m24 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m36 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[m   \u001b[36m48 \u001b[m\u001b[1m[\u001b[30m       0.0%\u001b[m]\u001b[14;3H\u001b[m\u001b[36mMem\u001b[m\u001b[1m[\u001b[m\u001b[32m|||||\u001b[34m|\u001b[33m|||||||||||\u001b[30m\u001b[1m   26.9G/187G\u001b[m]\u001b[m   \u001b[36mTasks: \u001b[1m6\u001b[m\u001b[36m, \u001b[32m\u001b[1m18\u001b[m\u001b[32m thr\u001b[36m; \u001b[32m\u001b[1m1\u001b[m\u001b[36m running\u001b[15;3HSwp\u001b[m\u001b[1m[\u001b[30m                         0K/0K\u001b[m]\u001b[m   \u001b[36mLoad average: \u001b[m\u001b[1m2.03 \u001b[36m2.11 \u001b[m\u001b[36m2.12 \u001b[16;41HUptime: \u001b[1m11 days, 19:11:21\u001b[19;23r\u001b[19;1H\u001b[2L\u001b[1;24r\u001b[2;7H\u001b[31m|\u001b[2;16H\u001b[30m\u001b[1m7\u001b[2;26H\u001b[m\u001b[31m|\u001b[2;35H\u001b[30m\u001b[1m7\u001b[4;26H\u001b[m\u001b[32m|\u001b[4;35H\u001b[30m\u001b[1m7\u001b[5;7H\u001b[m\u001b[32m|\u001b[5;16H\u001b[30m\u001b[1m7\u001b[5;26H\u001b[m\u001b[32m|\u001b[5;35H\u001b[30m\u001b[1m7\u001b[5;64H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     2\u001b[6;26H\u001b[m\u001b[31m|\u001b[6;35H\u001b[30m\u001b[1m7\u001b[6;45H\u001b[m\u001b[32m|\u001b[6;54H\u001b[30m\u001b[1m7\u001b[7;7H\u001b[m\u001b[32m|\u001b[6C\u001b[30m\u001b[1m1.3\u001b[7;26H\u001b[m\u001b[31m|\u001b[7;35H\u001b[30m\u001b[1m7\u001b[7;64H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     1.3\u001b[8;7H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     2.7\u001b[8;26H\u001b[m\u001b[31m|\u001b[6C\u001b[30m\u001b[1m1.3\u001b[8;64H\u001b[m\u001b[32m||||||98.0%\u001b[9;7H\u001b[31m|\u001b[9;16H\u001b[30m\u001b[1m7\u001b[9;45H\u001b[m\u001b[32m|\u001b[6C\u001b[30m\u001b[1m1.3\u001b[10;7H\u001b[m\u001b[32m|\u001b[10;16H\u001b[30m\u001b[1m7\u001b[10;64H\u001b[m\u001b[32m|\u001b[6C\u001b[30m\u001b[1m1.3\u001b[11;45H\u001b[m\u001b[32m|\u001b[11;54H\u001b[30m\u001b[1m7\u001b[12;7H\u001b[m\u001b[32m|\u001b[12;16H\u001b[30m\u001b[1m7\u001b[13;7H\u001b[m\u001b[32m|\u001b[13;16H\u001b[30m\u001b[1m7\u001b[16;65H\u001b[36m3\u001b[2;7H\u001b[30m\u001b[1m \u001b[2;16H0\u001b[2;26H\u001b[m\u001b[34m|\u001b[31m|\u001b[30m\u001b[1m     1.3\u001b[2;45H\u001b[m\u001b[32m|\u001b[2;54H\u001b[30m\u001b[1m7\u001b[2;64H\u001b[m\u001b[31m|\u001b[8C\u001b[30m\u001b[1m7\u001b[3;26H\u001b[m\u001b[31m|\u001b[3;35H\u001b[30m\u001b[1m7\u001b[4;7H\u001b[m\u001b[32m|\u001b[4;16H\u001b[30m\u001b[1m7\u001b[4;26H \u001b[4;35H0\u001b[4;64H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     2\u001b[5;7H\u001b[m\u001b[31m|\u001b[5;33H\u001b[30m\u001b[1m2.0\u001b[5;64H\u001b[m\u001b[31m|\u001b[30m\u001b[1m      1.3\u001b[6;26H\u001b[m\u001b[32m|\u001b[6;45H\u001b[31m|\u001b[6;64H\u001b[32m|\u001b[8C\u001b[30m\u001b[1m7\u001b[7;7H \u001b[6C0.0\u001b[7;26H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     2.0\u001b[7;64H       0.0\u001b[8;7H       0.0\u001b[8;26H\u001b[m\u001b[32m|\u001b[6C\u001b[30m\u001b[1m0.7\u001b[8;45H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     3.3\u001b[8;71H\u001b[m\u001b[32m4\u001b[31m.0%\u001b[9;7H\u001b[30m\u001b[1m \u001b[9;16H0\u001b[9;45H \u001b[6C0.0\u001b[10;7H \u001b[10;16H0\u001b[10;26H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     5.3\u001b[10;64H \u001b[6C0.0\u001b[11;7H\u001b[m\u001b[32m|\u001b[11;16H\u001b[30m\u001b[1m7\u001b[11;45H \u001b[11;54H0\u001b[11;64H\u001b[m\u001b[31m|\u001b[8C\u001b[30m\u001b[1m7\u001b[12;7H \u001b[12;16H0\u001b[13;7H\u001b[m\u001b[31m|\u001b[6C\u001b[30m\u001b[1m2\u001b[13;64H\u001b[m\u001b[32m|\u001b[31m|\u001b[30m\u001b[1m     1.3\u001b[16;65H\u001b[36m4\u001b[19;4H\u001b[m\u001b[30m\u001b[46m13\u001b[19;59H1.82\u001b[20;4H\u001b[m\u001b[m22\u001b[20;48H0\u001b[20;59H0.27 \u001b[32m/usr/bin/python3 \u001b[1;1H\u001b[m\u001b[m\u001b[?1000l\u001b[24;1H\u001b[2J\u001b[?47l\u001b8"
     ]
    }
   ],
   "source": [
    "!htop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 15 06:38:52 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   28C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/mnt/backup/temp_submission/models/'\n",
    "model_1 = os.path.join(model_dir, 'nnUNet/2d/')\n",
    "# fold_list = ['fold_0', 'fold_1', 'fold_2', 'fold_3', 'fold_4']\n",
    "# fold_list = [fold_0, fold_1, fold_2, fold_3, fold_4]\n",
    "\n",
    "Plan = 'Task555_PETCT/nnUNetTrainerV2_Loss_Dice__nnUNetPlansv2.1/'\n",
    "\n",
    "fold_all = os.path.join(model_1, Plan, 'all/')\n",
    "fold_0 = os.path.join(model_1, Plan, 'fold_0/')\n",
    "fold_1 = os.path.join(model_1, Plan, 'fold_1/')\n",
    "fold_2 = os.path.join(model_1, Plan, 'fold_2/')\n",
    "fold_3 = os.path.join(model_1, Plan, 'fold_3/')\n",
    "fold_4 = os.path.join(model_1, Plan, 'fold_4/')\n",
    "\n",
    "fold_list = [fold_0, fold_1, fold_2, fold_3, fold_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in fold_list:\n",
    "    if os.path.exists(fold):\n",
    "        shutil.rmtree(fold)\n",
    "        shutil.copytree(fold_all, fold)\n",
    "    else:\n",
    "        shutil.copytree(fold_all, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "['dataset']\n",
      "['23090580_20131226.hdf5', '23090618_20161212.hdf5', '23090628_20150204.hdf5', '23090643_20121227.hdf5', '23090644_20131216.hdf5', '23090636_20121018.hdf5', '23090585_20130213.hdf5', '23090572_20130226.hdf5', '23090603_20141212.hdf5', '23090601_20130225.hdf5', '23090569_20120607.hdf5', '23090599_20140701.hdf5', '23090597_20130227.hdf5', '23090627_20160608.hdf5', '23090559_20150812.hdf5', '23090614_20120402.hdf5', '23090557_20130717.hdf5', '23090634_20150409.hdf5', '23090581_20130626.hdf5', '23090626_20160119.hdf5', '23090568_20121018.hdf5', '23090623_20120406.hdf5', '23090615_20140403.hdf5', '23090640_20140711.hdf5', '23090613_20130208.hdf5', '23090641_20160510.hdf5', '23090609_20120510.hdf5', '23090560_20160114.hdf5', '23090563_20151216.hdf5', '23090596_20150112.hdf5', '23090632_20130807.hdf5', '23090598_20130103.hdf5', '23090584_20120523.hdf5', '23090633_20120403.hdf5', '23090607_20120420.hdf5', '23090594_20160706.hdf5', '23090622_20150105.hdf5', '23090583_20160308.hdf5', '23090637_20140401.hdf5', '23090620_20130617.hdf5', '23090645_20141212.hdf5', '23090621_20130409.hdf5', '23090562_20140206.hdf5', '23090582_20150401.hdf5', '23090566_20141114.hdf5', '23090571_20120517.hdf5', '23090642_20130409.hdf5', '23090595_20121015.hdf5', '23090586_20120627.hdf5', '23090604_20140303.hdf5', '23090561_20120330.hdf5', '23090610_20151210.hdf5', '23090639_20150522.hdf5', '23090608_20120718.hdf5', '23090588_20131025.hdf5', '23090578_20120613.hdf5', '23090593_20120625.hdf5', '23090564_20130312.hdf5', '23090631_20130128.hdf5', '23090616_20140331.hdf5', '23090606_20120619.hdf5', '23090638_20131126.hdf5', '23090629_20120830.hdf5', '23090592_20130218.hdf5', '23090625_20160111.hdf5', '23090589_20140219.hdf5', '23090617_20140211.hdf5', '23090590_20121212.hdf5', '23090619_20121210.hdf5', '23090611_20150212.hdf5', '23090612_20121213.hdf5', '23090646_20120718.hdf5', '23090600_20121108.hdf5', '23090579_20141215.hdf5', '23090630_20130213.hdf5', '23090558_20120330.hdf5', '23090591_20140124.hdf5', '23090567_20160819.hdf5', '23090635_20140710.hdf5', '23090587_20150908.hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('/mnt/dataset')))\n",
    "print(os.listdir('/mnt'))\n",
    "print(os.listdir('/mnt/dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Copy model..\n",
    "# shutil.copytree('/tf/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/3d_fullres/Task555_PETCT/nnUNetTrainerV2_Loss_Dice__nnUNetPlansv2.1/', '/tf/backup/temp_submission/models/nnUNet/3d_fullres/Task555_PETCT/nnUNetTrainerV2_Loss_Dice__nnUNetPlansv2.1/')\n",
    "# shutil.copytree('/tf/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/3d_fullres/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/', '/tf/sub_temp/models/nnUNet/3d_fullres/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1')\n",
    "# shutil.copytree('/tf/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2_Loss_CEGDL__nnUNetPlansv2.1/', '/tf/sub_temp/models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2_Loss_CEGDL__nnUNetPlansv2.1/')\n",
    "# shutil.copytree('/tf/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2_Loss_DiceTopK10__nnUNetPlansv2.1/', '/tf/sub_temp/models/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2_Loss_DiceTopK10__nnUNetPlansv2.1/')\n",
    "# shutil.copytree('/tf/backup/temp_submission/models/module/', '/tf/sub_temp/models/module')\n",
    "# shutil.copytree('/tf/submission/submitted/21-10-27_13:29:02-models/', '/tf/temp_submission/models')\n",
    "# shutil.copytree('/tf/backup/nnUNet', '/tf/temp_submission/models/module')\n",
    "# shutil.copytree('/tf/backup/temp_submission/', '/tf/sub_temp/')\n",
    "# shutil.copytree('/tf/temp_submission/', '/tf/backup/temp_submission')\n",
    "# shutil.copytree('/tf/sub_temp/', '/tf/sub_temp_temp/')\n",
    "# shutil.copytree('/tf/submission/submit_files', /tf/sub_temp_temp/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/tf/submission/submit_files/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/tf/temp/models/nnUNet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/tf/backup/temp_submission/models/nnUNet/2d/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/tf/backup/working/nnUNet/nnunet/nnUNet_trained_models/nnUNet/3d_fullres/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/mnt/dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
