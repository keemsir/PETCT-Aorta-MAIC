{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Obtaining file:///tf/temp_submission/models/module\n",
      "Requirement already satisfied: torch>=1.6.0a in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.9.1+cu111)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (4.62.3)\n",
      "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2.3.0)\n",
      "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.18.3)\n",
      "Requirement already satisfied: medpy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.4.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.7.1)\n",
      "Requirement already satisfied: batchgenerators>=0.23 in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.23)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.19.5)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (0.0)\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2.1.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (1.3.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from nnunet==1.7.0) (2.22.0)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (3.2.1)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.8/dist-packages (from nnunet==1.7.0) (2021.10.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0a->nnunet==1.7.0) (3.7.4.3)\n",
      "Requirement already satisfied: pydicom>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from dicom2nifti->nnunet==1.7.0) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (3.4.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14->nnunet==1.7.0) (8.3.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.0.0)\n",
      "Requirement already satisfied: unittest2 in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nnunet==1.7.0) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nnunet==1.7.0) (2.8.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from nibabel->nnunet==1.7.0) (21.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==1.7.0) (1.3.2)\n",
      "Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.15.0)\n",
      "Requirement already satisfied: argparse in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: traceback2 in /usr/local/lib/python3.8/dist-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)\n",
      "Requirement already satisfied: linecache2 in /usr/local/lib/python3.8/dist-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)\n",
      "Installing collected packages: nnunet\n",
      "  Attempting uninstall: nnunet\n",
      "    Found existing installation: nnunet 1.7.0\n",
      "    Can't uninstall 'nnunet'. No files were found to uninstall.\n",
      "  Running setup.py develop for nnunet\n",
      "Successfully installed nnunet\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def maybe_mkdir_p(directory: str) -> None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# maic_dir = '/mnt/backup/'\n",
    "# base_dir = os.path.join(maic_dir, 'working')\n",
    "base_dir = os.getcwd()\n",
    "input_dir = '/mnt/NM/dataset/'\n",
    "temp_dir = os.path.join(base_dir, 'models/temp/convert_data/')\n",
    "\n",
    "# maybe_mkdir_p(base_dir)\n",
    "maybe_mkdir_p(temp_dir)\n",
    "\n",
    "# ! git clone https://github.com/keemsir/nnUNet.git\n",
    "\n",
    "respository_dir = os.path.join(base_dir, 'models/module')\n",
    "os.chdir(respository_dir)\n",
    "\n",
    "! pip install -e .\n",
    "\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import special\n",
    "import copy\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "# must install\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Completed!\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Task55_PETCT'\n",
    "convert_name = 'Task555_PETCT'\n",
    "\n",
    "main_dir = os.path.join(base_dir, 'models/main/nnUNet/nnunet')\n",
    "mainT_dir = os.path.join(temp_dir, 'nnUNet/nnunet')\n",
    "\n",
    "rawbase_dir = os.path.join(mainT_dir, 'nnUNet_raw_data_base/')\n",
    "\n",
    "pp_dir = os.path.join(mainT_dir, 'preprocessed')\n",
    "tasks_dir = os.path.join(mainT_dir, 'Tasks')\n",
    "task_dir = os.path.join(tasks_dir, task_name)\n",
    "\n",
    "model_dir = os.path.join(main_dir, 'nnUNet_trained_models')\n",
    "Prediction_dir = os.path.join(main_dir, 'nnUNet_Prediction_Results')\n",
    "result_dir = os.path.join(Prediction_dir, convert_name)\n",
    "\n",
    "staple_dir = os.path.join(Prediction_dir, 'staple')\n",
    "\n",
    "# 1. Data preprocessing\n",
    "maybe_mkdir_p(tasks_dir)\n",
    "maybe_mkdir_p(temp_dir)\n",
    "\n",
    "# 2. Directory\n",
    "maybe_mkdir_p(main_dir)\n",
    "maybe_mkdir_p(model_dir)\n",
    "maybe_mkdir_p(pp_dir)\n",
    "\n",
    "# 3. Directory\n",
    "maybe_mkdir_p(result_dir)\n",
    "maybe_mkdir_p(staple_dir)\n",
    "\n",
    "\n",
    "#Environment Setting\n",
    "os.environ['nnUNet_raw_data_base'] = rawbase_dir #os.path.join(mainT_dir, 'nnUNet_raw_data_base')\n",
    "os.environ['nnUNet_preprocessed'] = pp_dir #os.path.join(mainT_dir, 'preprocessed')\n",
    "os.environ['RESULTS_FOLDER'] = './models' #os.path.join(main_dir, 'nnUNet_trained_models')\n",
    "\n",
    "print('Setting Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating \"Task55_PETCT\" Image & Label ..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/NM/dataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2c35373783bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image Patient : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mhdf2nifti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2c35373783bb>\u001b[0m in \u001b[0;36mhdf2nifti\u001b[0;34m(hdf_folder, save_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmaybe_mkdir_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'imagesTs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating \"{}\" Image & Label ..'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhdf5_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/NM/dataset/'"
     ]
    }
   ],
   "source": [
    "size_dic = {}\n",
    "def hdf2nifti(hdf_folder: str, save_folder: str):\n",
    "    # hdf_folder : [train_dir, test_dir] hdf5 file path\n",
    "    # save_folder : [imagesTr, imagesTs] Save Folder path\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'imagesTr'))\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'imagesTs'))\n",
    "    maybe_mkdir_p(os.path.join(save_folder, 'labelsTr'))\n",
    "    print('Creating \"{}\" Image & Label ..'.format(os.path.basename(os.path.normpath(save_folder))))\n",
    "    hdf5_files = os.listdir(hdf_folder)\n",
    "\n",
    "\n",
    "    for hdf5_file in hdf5_files:\n",
    "\n",
    "\n",
    "        hdf5_path = os.path.join(hdf_folder, hdf5_file)\n",
    "\n",
    "        # image\n",
    "        f_i = h5py.File(hdf5_path, 'r')\n",
    "        ctarr = np.asarray(f_i['CT'])\n",
    "        petarr = np.asarray(f_i['PET'])\n",
    "        sizearr = np.asarray(f_i['Size'])\n",
    "        f_i.close()\n",
    "        \n",
    "        size_dic[hdf5_file[:17]] = sizearr # new\n",
    "\n",
    "        SLICE_SIZE_X, SLICE_SIZE_Y, SLICE_COUNT = ctarr.shape\n",
    "        images = np.empty([SLICE_SIZE_X, SLICE_SIZE_Y, SLICE_COUNT, 0], dtype=np.single)\n",
    "\n",
    "        image_ct = np.expand_dims(ctarr, axis=3)\n",
    "        images = np.append(images, image_ct, axis=3)\n",
    "        image_pet = np.expand_dims(petarr, axis=3)\n",
    "        images = np.append(images, image_pet, axis=3)\n",
    "\n",
    "        niim = nib.Nifti1Image(images, affine=np.eye(4))\n",
    "        nib.save(niim, os.path.join(save_folder, 'imagesTs/{}.nii.gz'.format(hdf5_file[:17])))\n",
    "\n",
    "\n",
    "    print('\"{}\" Image & Label Completed !!'.format(os.path.basename(os.path.normpath(save_folder))))\n",
    "    print('Image Patient : {}'.format(len(os.listdir(input_dir))))\n",
    "\n",
    "hdf2nifti(input_dir, task_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/temp/nnUNet/nnunet/Tasks/Task55_PETCT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-104991aedf2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/temp/nnUNet/nnunet/Tasks/Task55_PETCT'"
     ]
    }
   ],
   "source": [
    "def json_mk(save_dir: str):\n",
    "    # Path\n",
    "    imagesTr = os.path.join(save_dir, 'imagesTr')\n",
    "    imagesTs = os.path.join(save_dir, 'imagesTs')\n",
    "    maybe_mkdir_p(imagesTr)\n",
    "    maybe_mkdir_p(imagesTs)\n",
    "\n",
    "    overwrite_json_file = True\n",
    "    json_file_exist = False\n",
    "\n",
    "    if os.path.exists(os.path.join(save_dir, 'dataset.json')):\n",
    "        print('dataset.json already exist!')\n",
    "        json_file_exist = True\n",
    "\n",
    "    if json_file_exist == False or overwrite_json_file:\n",
    "\n",
    "        json_dict = OrderedDict()\n",
    "        json_dict['name'] = \"PETCT\"\n",
    "        json_dict['description'] = \"Medical Image AI Challenge 2021\"\n",
    "        json_dict['tensorImageSize'] = \"4D\"\n",
    "        json_dict['reference'] = \"https://maic.or.kr/competitions/\"\n",
    "        json_dict['licence'] = \"SNUH\"\n",
    "        json_dict['release'] = \"18/10/2021\"\n",
    "\n",
    "        json_dict['modality'] = {\n",
    "            \"0\": \"CT\",\n",
    "            \"1\": \"PET\"\n",
    "        }\n",
    "        json_dict['labels'] = {\n",
    "            \"0\": \"background\",\n",
    "            \"1\": \"Aorta\"\n",
    "        }\n",
    "\n",
    "        train_ids = sorted(os.listdir(imagesTr))\n",
    "        test_ids = sorted(os.listdir(imagesTs))\n",
    "        json_dict['numTraining'] = len(train_ids)\n",
    "        json_dict['numTest'] = len(test_ids)\n",
    "\n",
    "        json_dict['training'] = [{'image': \"./imagesTr/%s\" % i, \"label\": \"./labelsTr/%s\" % i} for i in train_ids]\n",
    "\n",
    "        json_dict['test'] = [\"./imagesTs/%s\" % i for i in test_ids] #(i[:i.find(\"_0000\")])\n",
    "\n",
    "        with open(os.path.join(save_dir, \"dataset.json\"), 'w') as f:\n",
    "            json.dump(json_dict, f, indent=4, sort_keys=False)\n",
    "\n",
    "        if os.path.exists(os.path.join(save_dir, 'dataset.json')):\n",
    "            if json_file_exist == False:\n",
    "                print('dataset.json created!')\n",
    "            else:\n",
    "                print('dataset.json overwritten!')\n",
    "json_mk(task_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!nnUNet_convert_decathlon_task -i ./models/temp/convert_data/nnUNet/nnunet/Tasks/Task55_PETCT -output_task_id 555 # -i : task_dir\n",
    "# !nnUNet_plan_and_preprocess -t 555 # --verify_dataset_integrity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "using model stored in  ./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1\n",
      "This model expects 2 input modalities for each image\n",
      "Found 1 unique case ids, here are some examples: ['23010018_20141226']\n",
      "If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\n",
      "number of cases: 1\n",
      "number of cases that still need to be predicted: 1\n",
      "emptying cuda cache\n",
      "loading parameters for folds, None\n",
      "folds is None so we will automatically look for output folders (not using 'all'!)\n",
      "found the following folds:  ['./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4']\n",
      "using the following model files:  ['./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', './model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']\n",
      "starting preprocessing generator\n",
      "starting prediction...\n",
      "preprocessing ./23010018_20141226.nii.gz\n",
      "using preprocessor PreprocessorFor2D\n",
      "before crop: (2, 284, 200, 200) after crop: (2, 284, 173, 173) spacing: [1. 1. 1.] \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (2, 284, 173, 173)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (2, 284, 173, 173)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "(2, 284, 173, 173)\n",
      "This worker has ended successfully, no errors to report\n",
      "predicting ./23010018_20141226.nii.gz\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "debug: mirroring False mirror_axes (0, 1)\n",
      "inference done. Now waiting for the segmentation export to finish...\n",
      "force_separate_z: None interpolation order: 1\n",
      "no resampling necessary\n",
      "WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run consolidate_folds in the output folder of the model first!\n",
      "The folder you need to run this in is ./model/nnUNet/2d/Task555_PETCT/nnUNetTrainerV2__nnUNetPlansv2.1\n"
     ]
    }
   ],
   "source": [
    "!nnUNet_predict -i ./models/temp/convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs -o ./models/result/2d -t 555 -tr nnUNetTrainerV2 -m 2d --disable_tta\n",
    "!nnUNet_predict -i ./models/temp/convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs -o ./models/result/3d_fullres -t 555 -tr nnUNetTrainerV2 -m 3d_fullres --disable_tta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "import os\n",
    "import copy\n",
    "import nibabel as nib\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Utils ..\n",
    "def subdirs(folder: str, join: bool = True, prefix: str = None, suffix: str = None, sort: bool = True) -> List[str]:\n",
    "    if join:\n",
    "        l = os.path.join\n",
    "    else:\n",
    "        l = lambda x, y: y\n",
    "    res = [l(folder, i) for i in os.listdir(folder) if os.path.isdir(os.path.join(folder, i))\n",
    "           and (prefix is None or i.startswith(prefix))\n",
    "           and (suffix is None or i.endswith(suffix))]\n",
    "    if sort:\n",
    "        res.sort()\n",
    "    return res\n",
    "\n",
    "def subfiles(folder: str, join: bool = True, prefix: str = None, suffix: str = None, sort: bool = True) -> List[str]:\n",
    "    if join:\n",
    "        l = os.path.join\n",
    "    else:\n",
    "        l = lambda x, y: y\n",
    "    res = [l(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))\n",
    "           and (prefix is None or i.startswith(prefix))\n",
    "           and (suffix is None or i.endswith(suffix))]\n",
    "    if sort:\n",
    "        res.sort()\n",
    "    return res\n",
    "\n",
    "def maybe_mkdir_p(directory: str) -> None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def find3d_ind(bigMask_):\n",
    "    indV = np.argwhere(bigMask_ == True)\n",
    "    indVx = indV[:, 0]\n",
    "    indVy = indV[:, 1]\n",
    "    indVz = indV[:, 2]\n",
    "\n",
    "    xMin = np.min(indVx)\n",
    "    xMax = np.max(indVx)\n",
    "    yMin = np.min(indVy)\n",
    "    yMax = np.max(indVy)\n",
    "    zMin = np.min(indVz)\n",
    "    zMax = np.max(indVz)\n",
    "    return xMin, xMax, yMin, yMax, zMin, zMax\n",
    "\n",
    "\n",
    "def staple_wjcheon(D, iterlim, p, q):\n",
    "    # ---- inputs:\n",
    "    # *D: a matrix of N(voxels) x R(binary decisions by experts)\n",
    "    # *p: intial sensitivity\n",
    "    # *q: intial specificity\n",
    "    # *iterlim: iteration limit\n",
    "    # ---- outputs:\n",
    "    # *p: final sensitivity estimate\n",
    "    # *q: final specificity estimate\n",
    "    # *W: estimated belief in true segmentation\n",
    "    [N, R] = np.shape(D)\n",
    "    Tol = 1e-4\n",
    "    iter = 0\n",
    "    gamma = np.sum(np.sum(D, axis=0) / (R * N))\n",
    "    W = np.zeros((N, 1), dtype=np.single)\n",
    "    S0 = np.sum(W)\n",
    "\n",
    "    stapleV = []\n",
    "    sen = []\n",
    "    spec = []\n",
    "    Sall = []\n",
    "    while (True):\n",
    "        iter = iter + 1\n",
    "        Sall.append(S0)\n",
    "\n",
    "        ind1 = np.equal(D, 1)\n",
    "        ind0 = np.equal(D, 0)\n",
    "        ind1_not = np.logical_not(ind1)\n",
    "        ind0_not = np.logical_not(ind0)\n",
    "\n",
    "        p = np.repeat(p, N, axis=0)\n",
    "        p1 = copy.deepcopy(p)\n",
    "        p0 = copy.deepcopy(1 - p1)\n",
    "\n",
    "        p1[ind1_not] = 1\n",
    "        p0[ind0_not] = 1\n",
    "        a = gamma * np.multiply(np.prod(p1, axis=1), np.prod(p0, axis=1))\n",
    "        del p1, p0\n",
    "\n",
    "        q = np.repeat(q, N, axis=0)\n",
    "        q0 = copy.deepcopy(q)\n",
    "        q1 = copy.deepcopy(1 - q0)\n",
    "        q1[ind1_not] = 1\n",
    "        q0[ind0_not] = 1\n",
    "        del ind1, ind0, ind1_not, ind0_not\n",
    "        b = (1 - gamma) * np.multiply(np.prod(q0, axis=1), np.prod(q1, axis=1))\n",
    "        del q1, q0\n",
    "\n",
    "        W = np.divide(a, a + b)\n",
    "        W = np.reshape(W, (1, len(W)))\n",
    "\n",
    "        del a, b, p, q\n",
    "\n",
    "        p = np.divide(np.matmul(W, D), np.sum(W))\n",
    "        q = np.divide(np.matmul(1 - W, 1 - D), np.sum(1 - W))\n",
    "        # Check convergence\n",
    "        S = np.sum(W)\n",
    "        if np.abs(S - S0) < Tol:\n",
    "            print(\"STAPLE converged in {} iterations\".format(iter))\n",
    "            break\n",
    "        else:\n",
    "            S0 = S\n",
    "\n",
    "        # Check iteration limit\n",
    "        if (iter > iterlim):\n",
    "            print(\"STAPLE: Number of iterations exceeded without convergence (convergence tolerance = %e)\".format(Tol))\n",
    "            break\n",
    "\n",
    "    return W, p, q, Sall\n",
    "\n",
    "\n",
    "def getUniformScanXYZVals_standardalone(rtstStruct):\n",
    "    sizeArray = np.shape(rtstStruct)\n",
    "    sizeDim1 = sizeArray[0] - 1  # sizeArray[0] - 1\n",
    "    sizeDim2 = sizeArray[1] - 1  # sizeArray[1] - 1\n",
    "\n",
    "    xOffset = 0\n",
    "    yOffset = 0\n",
    "    firstZValue = 0\n",
    "    grid2Units = 4.6875  # 0.9765625\n",
    "    grid1Units = 4.6875  # 0.9765625\n",
    "    sliceThickness = 3.27001953\n",
    "\n",
    "    # xVals = xOffset - (sizeDim2*grid2Units)/2 : grid2Units :\n",
    "    xSt = xOffset - (sizeDim2 * grid2Units) / 2\n",
    "    xEnd = xOffset + (sizeDim2 * grid2Units) / 2 + grid2Units\n",
    "    xVals = np.arange(xSt, xEnd, grid2Units)\n",
    "\n",
    "    ySt = yOffset - (sizeDim1 * grid1Units) / 2\n",
    "    yEnd = yOffset + (sizeDim1 * grid1Units) / 2 + grid2Units\n",
    "    yVals = np.arange(ySt, yEnd, grid1Units)\n",
    "    yVals = np.flip(yVals)\n",
    "\n",
    "    nZSlices = sizeArray[2];\n",
    "    zSt = firstZValue\n",
    "    zEnd = sliceThickness * (nZSlices - 1) + firstZValue + sliceThickness\n",
    "    zVals = np.arange(zSt, zEnd, sliceThickness)\n",
    "\n",
    "    return xVals, yVals, zVals\n",
    "\n",
    "\n",
    "def kappa_stats(D, ncat):\n",
    "    [N, M] = np.shape(D)\n",
    "    lk = len(ncat)\n",
    "    x = []\n",
    "    for iterVal in range(0, lk):\n",
    "        x.append(np.sum(np.equal(D, ncat[iterVal]), axis=1))\n",
    "    x = np.transpose(x)\n",
    "\n",
    "    p = np.divide(np.sum(x, axis=0), (N * M)) # Default : axis=0\n",
    "    eps = np.finfo(float).eps\n",
    "    k_a = np.sum(np.multiply(x, M - x), axis=0) # Default : axis=0\n",
    "    k_b = (N * M * (M - 1)) * np.multiply(p, (1 - p)) + eps\n",
    "    k = 1 - np.divide(k_a, k_b)\n",
    "    sek = np.sqrt(2 / (N * M * (M - 1)))\n",
    "    pk = drxlr_get_p_gaussian(np.divide(k, sek)) / 2\n",
    "    kappa_a = N * M * M - np.sum(np.sum(np.multiply(x, x)))\n",
    "    kappa_b = N * M * (M - 1) * np.sum(np.multiply(p, (1 - p))) + eps\n",
    "    kappa = 1 - (kappa_a / kappa_b)\n",
    "    sekappa_a = np.sum(np.multiply(p, (1 - p)) * np.sqrt(N * M * (M - 1)) + eps)\n",
    "    sekappa_b = np.power(np.sqrt(np.sum(np.multiply(p, (1 - p)))), 2) - np.sum(\n",
    "        np.multiply(np.multiply(p, 1 - p), (1 - 2 * p)))\n",
    "    sekappa = np.sqrt(2) / sekappa_a * sekappa_b\n",
    "    z = kappa / sekappa\n",
    "    pval = drxlr_get_p_gaussian(z) / 2\n",
    "\n",
    "    return kappa, pval, k, pk\n",
    "\n",
    "\n",
    "def drxlr_get_p_gaussian(x):\n",
    "    p = special.erfc(np.abs(x) / np.sqrt(2))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def calConsensus_standardalone(rtstStructs):\n",
    "    keysDictionary = list(rtstStructs.keys())\n",
    "    bigMask = rtstStructs.get(keysDictionary[0])\n",
    "    dictionaryLength = len(rtstStructs)\n",
    "    for iter1 in range(0, dictionaryLength):\n",
    "        bigMask = np.logical_or(bigMask, rtstStructs.get(keysDictionary[iter1]))\n",
    "\n",
    "    iMin, iMax, jMin, jMax, kMin, kMax = find3d_ind(bigMask);\n",
    "\n",
    "    averageMask3M = np.zeros((iMax - iMin + 1, jMax - jMin + 1, kMax - kMin + 1), dtype=np.single)\n",
    "    rateMat = []\n",
    "    for iter1 in range(0, dictionaryLength):\n",
    "        mask3M = rtstStructs.get(keysDictionary[iter1])\n",
    "        mask3M_ROI = np.asanyarray(mask3M[iMin:iMax + 1, jMin: jMax + 1, kMin: kMax + 1])\n",
    "        averageMask3M = averageMask3M + mask3M_ROI\n",
    "        mask3M_ROI_flat = mask3M_ROI.flatten()\n",
    "        rateMat.append(mask3M_ROI_flat)\n",
    "    averageMask3M = averageMask3M / dictionaryLength\n",
    "    rateMat = np.transpose(rateMat)\n",
    "    scanNum = 1\n",
    "    iterlim = 100\n",
    "    senstart = 0.9999 * np.ones((1, dictionaryLength))\n",
    "    specstart = 0.9999 * np.ones((1, dictionaryLength))\n",
    "    [stapleV, sen, spec, Sall] = staple_wjcheon(rateMat, iterlim, np.single(senstart), np.single(specstart))\n",
    "\n",
    "    mean_sen = np.mean(sen)\n",
    "    std_sen = np.std(sen, ddof=1)\n",
    "    mean_spec = np.mean(spec)\n",
    "    std_spec = np.std(spec, ddof=1)\n",
    "\n",
    "    [xUnifV, yUnifV, zUnifV] = getUniformScanXYZVals_standardalone(mask3M)\n",
    "    vol = (xUnifV[2] - xUnifV[1]) * (yUnifV[1] - yUnifV[2]) * (zUnifV[2] - zUnifV[1])\n",
    "    vol = vol * 0.001\n",
    "\n",
    "    numBins = 20\n",
    "    obsAgree = np.linspace(0.001, 1, numBins)\n",
    "    rater_prob = np.mean(rateMat, axis=0)\n",
    "    chance_prob = np.sqrt(np.multiply(rater_prob, (1 - rater_prob)))\n",
    "    chance_prob = np.reshape(chance_prob, (1, np.shape(chance_prob)[0]))\n",
    "    chance_prob_mat = np.repeat(chance_prob, np.shape(rateMat)[0], axis=0)\n",
    "    reliabilityV = np.mean(np.divide((rateMat - chance_prob_mat), (1 - chance_prob_mat)), axis=1)\n",
    "    del rater_prob, chance_prob, chance_prob_mat\n",
    "\n",
    "    volV = []\n",
    "    volStapleV = []\n",
    "    volKappaV = []\n",
    "    for iter10 in range(0, len(obsAgree)):\n",
    "        updatedValue = np.sum((averageMask3M.flatten() >= obsAgree[iter10]) * vol)\n",
    "        volV.append(updatedValue)\n",
    "        updatedValue2 = np.sum((stapleV.flatten() >= obsAgree[iter10]) * vol)\n",
    "        volStapleV.append(updatedValue2)\n",
    "        updatedValue3 = np.sum((reliabilityV.flatten() >= obsAgree[iter10]) * vol)\n",
    "        volKappaV.append(updatedValue3)\n",
    "\n",
    "    # calculate overall kappa\n",
    "    [kappa, pval, k, pk] = kappa_stats(rateMat, [0, 1])\n",
    "    min_vol = np.min(np.sum(rateMat, axis=0)) * vol\n",
    "    max_vol = np.max(np.sum(rateMat, axis=0)) * vol\n",
    "    mean_vol = np.mean(np.sum(rateMat, axis=0)) * vol\n",
    "    sd_vol = np.std(np.sum(rateMat, axis=0), ddof=1) * vol\n",
    "\n",
    "    print('-------------------------------------------')\n",
    "    print('Overall kappa: {0:1.8f}'.format(kappa))\n",
    "    print('p-value: {0:1.8f}'.format(pval))\n",
    "    print('Mean Sensitivity: {0:1.8f}'.format(mean_sen))\n",
    "    print('Std. Sensitivity: {0:1.8f}'.format(std_sen))\n",
    "    print('Mean Specificity: {0:1.8f}'.format(mean_spec))\n",
    "    print('Std. Specificity: {0:1.8f}'.format(std_spec))\n",
    "    print('Min. volume: {0:1.8f}'.format(min_vol))\n",
    "    print('Max. volume: {0:1.8f}'.format(max_vol))\n",
    "    print('Mean volume: {0:1.8f}'.format(mean_vol))\n",
    "    print('Std. volume: {0:1.8f}'.format(sd_vol))\n",
    "    print('Intersection volume: {0:1.8f}'.format(volV[-1]))\n",
    "    print('Union volume: {0:1.8f}'.format(volV[1]))\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "    len_x, len_y, len_z = np.shape(averageMask3M)\n",
    "    stapleV_reshape = np.reshape(stapleV, (len_x, len_y, len_z))\n",
    "    staple3M = np.zeros_like(bigMask, dtype=np.single)\n",
    "    staple3M[iMin:iMax + 1, jMin: jMax + 1, kMin: kMax + 1] = stapleV_reshape\n",
    "    #\n",
    "    reliabilityV_reshape = np.reshape(reliabilityV, (len_x, len_y, len_z))\n",
    "    reliability3M = np.zeros_like(bigMask, dtype=np.single)\n",
    "    reliability3M[iMin:iMax + 1, jMin: jMax + 1, kMin: kMax + 1] = reliabilityV_reshape\n",
    "    #\n",
    "    apparent3M = np.zeros_like(bigMask, dtype=np.single)\n",
    "    apparent3M[iMin:iMax + 1, jMin: jMax + 1, kMin: kMax + 1] = averageMask3M\n",
    "\n",
    "    return apparent3M, staple3M, reliability3M\n",
    "\n",
    "\n",
    "##### Parameter #####\n",
    "\n",
    "# folderlist_temp = ['21-11-03_13:12:07-models', '21-11-01_13:53:00-models']\n",
    "\n",
    "mainPath = './models/result/'\n",
    "savePath = './models/staple/'\n",
    "\n",
    "maybe_mkdir_p(savePath)\n",
    "\n",
    "folderList = subdirs(mainPath, join=False) # os.listdir(mainPath)\n",
    "\n",
    "result_list = subfiles(os.path.join(mainPath, folderList[0]), join=False, suffix='.nii.gz')\n",
    "\n",
    "targetWeight = 0.6\n",
    "\n",
    "print(\"STAPLE process is starting...\")\n",
    "\n",
    "\n",
    "# PatientKey = os.listdir(mainPath) # mainPath : Patient Key List\n",
    "\n",
    "\n",
    "for iterPatient in result_list: # PatientKey\n",
    "\n",
    "    dataPerPatient_aorta = {} # Model 별 Stack\n",
    "\n",
    "    # PatientKey = PatientKey + 'hdf5'\n",
    "\n",
    "    for ModelList in folderList:\n",
    "        # print(ModelList)\n",
    "\n",
    "        # patientStack_aorta = []\n",
    "\n",
    "        patientStack_aorta = np.array(nib.load(os.path.join('./models/result/{}'.format(ModelList), iterPatient)).dataobj)\n",
    "\n",
    "        dataPerPatient_aorta[ModelList] = patientStack_aorta\n",
    "\n",
    "    # STAPLE\n",
    "    [apparent3M_label2, staple3M_label2, reliability3M_label2] = calConsensus_standardalone(dataPerPatient_aorta)\n",
    "    mask2 = np.uint8((staple3M_label2 >= targetWeight))\n",
    "\n",
    "    nii_stp = nib.Nifti1Image(mask2, affine=np.eye(4))\n",
    "    nib.save(nii_stp, os.path.join(savePath, iterPatient))\n",
    "\n",
    "print(\"STAPLE process is done...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suv_params(ptarr, roi):\n",
    "    roi = np.asarray(roi>0, dtype=np.float)\n",
    "    suvmax = np.max(ptarr*roi)\n",
    "    suvmean = np.sum(ptarr*roi)/np.sum(roi)\n",
    "    return suvmax, suvmean\n",
    "\n",
    "def get_vol_params(ptzoom, roi):\n",
    "    roi = np.asarray(roi>0, dtype=np.float)\n",
    "    return np.prod(ptzoom) * np.sum(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_list = os.listdir('./models/staple/')\n",
    "PN_list.sort()\n",
    "\n",
    "for pn_l in PN_list:\n",
    "    _, ext = os.path.splitext(pn_l)\n",
    "    if ext == '.gz':\n",
    "        \n",
    "        pn = pn_l[:-7]\n",
    "        PATIENT_NUM = pn\n",
    "\n",
    "        ptarr = np.array(nib.load('./models/temp/convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs/{}_0001.nii.gz'.format(pn)).dataobj)\n",
    "        ctarr = np.array(nib.load('./models/temp/convert_data/nnUNet/nnunet/nnUNet_raw_data_base/nnUNet_raw_data/Task555_PETCT/imagesTs/{}_0000.nii.gz'.format(pn)).dataobj)\n",
    "        pred_arr = np.array(nib.load('./models/staple/{}.nii.gz'.format(pn)).dataobj)\n",
    "\n",
    "        # size = ([4.07283, 4.07283, 3.])\n",
    "        size = ([4.6875, 4.6875, 3.27001953])\n",
    "        # size = size_dic[PATIENT_NUM]\n",
    "        \n",
    "        #Calculate Mean SUV and Max SUV\n",
    "\n",
    "        suvmax, suvmean = get_suv_params(ptarr, pred_arr)\n",
    "\n",
    "        #Calculate Volume\n",
    "\n",
    "        aorvol = get_vol_params(size, pred_arr)\n",
    "        \n",
    "        PATIENT_NUM = pn\n",
    "\n",
    "        data = {'case' : [PATIENT_NUM], 'PD_Aorta_volume' : [aorvol], 'PD_SUVmean' : [suvmean]}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # .to_csv \n",
    "        if not os.path.exists('submission.csv'):\n",
    "            df.to_csv('submission.csv', index=False, mode='w')\n",
    "        else:\n",
    "            df.to_csv('submission.csv', index=False, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
